import asyncio
from pathlib import Path

from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_ollama import ChatOllama
from langchain_text_splitters import RecursiveCharacterTextSplitter

from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import (
    RunnableParallel, RunnablePassthrough, RunnableLambda
)
from langchain_core.runnables.history import RunnableWithMessageHistory


# ä¸­æ–‡Tokenä¼°ç®—å‡½æ•°
def count_chinese_tokens(text: str) -> int:
    chinese_chars = sum('\u4e00' <= char <= '\u9fff' for char in text)
    other_chars = len(text) - chinese_chars
    return int(chinese_chars * 1.33 + other_chars * 0.25)


# åŠ è½½PDFå¹¶æ„å»ºå‘é‡åº“
def init_knowledge_base(directory="docs"):
    documents = []
    for pdf_file in Path(directory).glob("*.pdf"):
        if pdf_file.is_file():
            try:
                loader = PyPDFLoader(str(pdf_file))
                documents.extend(loader.load())
            except Exception as e:
                print(f"è·³è¿‡æ–‡ä»¶ {pdf_file.name}ï¼ŒåŸå› : {e}")
                continue

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=count_chinese_tokens
    )
    splits = text_splitter.split_documents(documents)

    embeddings = HuggingFaceEmbeddings(
        model_name="BAAI/bge-small-zh-v1.5",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )

    vectorstore = FAISS.from_documents(splits, embeddings)
    return vectorstore.as_retriever(search_type="mmr", search_kwargs={"k": 5})


# æ„å»ºLangChain QAé“¾
def create_qa_chain(retriever):
    llm = ChatOllama(model="deepseek-r1:14b")

    prompt = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€ä¸ªä¹¦ç±å†…å®¹åˆ†æåŠ©æ‰‹ï¼Œè¯·æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡å†…å®¹æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "å…³äºä¹¦ç±å†…å®¹çš„é—®é¢˜ï¼š{input}")
    ])

    def extract_text(input_dict):
        return input_dict["input"] if isinstance(input_dict, dict) else input_dict

    # ä¸» QA ä»»åŠ¡é“¾
    qa_chain = (
        RunnableParallel({
            "context": RunnablePassthrough() | RunnableLambda(extract_text) | retriever,
            "input": RunnablePassthrough() | RunnableLambda(extract_text),
            "chat_history": RunnablePassthrough().with_config(configurable={"key": "chat_history"})
        }).with_config(runnables_have_config=True)  # âœ… å…³é”®ä¿®å¤
        | prompt
        | llm
        | StrOutputParser()
    )

    # åŠ å…¥ Memory æ”¯æŒ
    return RunnableWithMessageHistory(
        qa_chain,
        lambda session_id: ChatMessageHistory(),
        input_messages_key="input",
        history_messages_key="chat_history"
    )


# å¼‚æ­¥å¯¹è¯ä¸»æµç¨‹
async def chat_with_memory():
    retriever = init_knowledge_base()
    qa_chain = create_qa_chain(retriever)

    print("ğŸ¤– æœ¬åœ°çŸ¥è¯†åº“é—®ç­”æœºå™¨äººå·²å¯åŠ¨ï¼Œè¾“å…¥ 'é€€å‡º' ç»“æŸä¼šè¯ã€‚")
    while True:
        try:
            user_input = await asyncio.to_thread(input, "ä½ ï¼š")
            if user_input.lower().strip() in ('exit', 'quit', 'é€€å‡º'):
                break

            print("AIï¼š", end="", flush=True)
            result = await qa_chain.ainvoke(
                {"input": user_input},
                config={"configurable": {"session_id": "fixed_session"}}
            )
            print(result.strip())

        except KeyboardInterrupt:
            print("\nå¯¹è¯ä¸­æ­¢")
            break


if __name__ == "__main__":
    asyncio.run(chat_with_memory())
